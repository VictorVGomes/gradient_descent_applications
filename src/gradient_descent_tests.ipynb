{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib, matplotlib.pyplot as plt, os\n",
    "import imageio\n",
    "from gradient_descent.gradient_descent import Gradient_Descent, generate_gif\n",
    "from linear_regression.linear_regression_utils import mse_loss, mse_loss_, lm_grad_f\n",
    "# matplotlib.use(\"Agg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial betas: [[0.48951662]\n",
      " [0.23879586]]\n",
      "Reached convergence after 1135 steps.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[35.07930415],\n",
       "       [10.06884827]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "n = 1_000\n",
    "X = np.random.randn(n).reshape(-1, 1)\n",
    "X = np.concatenate((np.ones_like(X), X), axis=1)\n",
    "y = X[:, 1].reshape(-1, 1) * 10 + 35 + np.random.randn(n).reshape(-1, 1) * 3\n",
    "\n",
    "\n",
    "\n",
    "optimal_betas = np.linalg.inv(X.T.dot(X)).dot(X.T.dot(y))\n",
    "\n",
    "betas = np.random.randn(2).reshape(-1, 1)\n",
    "print(f\"Initial betas: {betas}\")\n",
    "\n",
    "w = Gradient_Descent(\n",
    "    weights=betas,\n",
    "    gradient_function=lm_grad_f,\n",
    "    epsilon=1e-5,\n",
    "    # loss_function=mse_loss, \n",
    "    # comment this parameter if you don't wish to use linear search (slower convergence)\n",
    ")\n",
    "\n",
    "w.fit(X=X, y=y, tau=1 / 2)\n",
    "\n",
    "# generate_gif(X, y, mse_loss_, w, gif_name='linear_regression_gradient_descent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached convergence after 20 steps.\n"
     ]
    }
   ],
   "source": [
    "betas = np.ones([2, 1])\n",
    "w = Gradient_Descent(\n",
    "    weights=betas,\n",
    "    gradient_function=lm_grad_f,\n",
    "    epsilon=1e-1,\n",
    "    loss_function=mse_loss, \n",
    "    # comment this parameter if you don't wish to use linear search (slower convergence)\n",
    ")\n",
    "\n",
    "w.fit(X=X, y=y, tau = 2/5,)\n",
    "generate_gif(X, y, mse_loss_, w, gif_name='linear_regression_gradient_descent_with_line_search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_stuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
